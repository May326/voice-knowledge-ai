import gradio as gr
import chromadb
from chromadb.utils import embedding_functions
import os
from pathlib import Path
import PyPDF2
import docx
import whisper
import edge_tts
import asyncio
import tempfile

# ===== LLM é…ç½® =====
LLM_TYPE = "GEMINI"  # å¯é€‰: "GEMINI", "OPENAI", "OLLAMA"

if LLM_TYPE == "GEMINI":
    import google.generativeai as genai
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY", ""))
    gemini_model = genai.GenerativeModel('gemini-2.5-flash-lite')  # ä½¿ç”¨æœ€æ–°çš„å…è´¹æ¨¡å‹
elif LLM_TYPE == "OPENAI":
    from openai import OpenAI
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
else:  # OLLAMA
    try:
        import ollama
    except:
        print("âš ï¸  è¯·å®‰è£… ollama: pip install ollama")

class EnglishConversationAI:
    def __init__(self):
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.kb_path = "./data/chroma_db"
        os.makedirs(self.kb_path, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=self.kb_path)
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        try:
            self.collection = self.client.get_collection(
                name="knowledge_base",
                embedding_function=self.embedding_fn
            )
        except:
            self.collection = self.client.create_collection(
                name="knowledge_base",
                embedding_function=self.embedding_fn
            )
        
        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.whisper_model = None
        
        # TTSè¯­éŸ³é…ç½® - è‹±æ–‡è¯­éŸ³
        self.tts_voices = {
            "ç¾å¼å¥³å£°": "en-US-AriaNeural",
            "ç¾å¼ç”·å£°": "en-US-GuyNeural",
            "è‹±å¼å¥³å£°": "en-GB-SoniaNeural",
            "è‹±å¼ç”·å£°": "en-GB-RyanNeural",
        }
        self.current_voice = "en-US-AriaNeural"
        
        # å¯¹è¯å†å²
        self.conversation_history = []
    
    def load_whisper(self):
        """å»¶è¿ŸåŠ è½½Whisperæ¨¡å‹"""
        if self.whisper_model is None:
            print("æ­£åœ¨åŠ è½½Whisperæ¨¡å‹...")
            self.whisper_model = whisper.load_model("base")
            print("Whisperæ¨¡å‹åŠ è½½å®Œæˆ")
        return self.whisper_model
    
    def transcribe_audio(self, audio_path):
        """è¯­éŸ³è½¬æ–‡å­— - è‹±æ–‡è¯†åˆ«"""
        if not audio_path:
            return ""
        
        try:
            model = self.load_whisper()
            # æŒ‡å®šä¸ºè‹±æ–‡è¯†åˆ«
            result = model.transcribe(audio_path, language="en")
            return result["text"]
        except Exception as e:
            return f"Speech recognition error: {str(e)}"
    
    async def text_to_speech_async(self, text, voice=None):
        """æ–‡å­—è½¬è¯­éŸ³ï¼ˆå¼‚æ­¥ï¼‰- è‹±æ–‡"""
        if not text or text.startswith("Error") or text.startswith("âŒ"):
            return None
        
        try:
            # é™åˆ¶è¯­éŸ³é•¿åº¦
            if len(text) > 800:
                text = text[:800] + "..."
            
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            output_path = temp_file.name
            temp_file.close()
            
            voice_to_use = voice if voice else self.current_voice
            communicate = edge_tts.Communicate(text, voice_to_use)
            await communicate.save(output_path)
            
            return output_path
        except Exception as e:
            print(f"TTS Error: {str(e)}")
            return None
    
    def text_to_speech(self, text, voice=None):
        """æ–‡å­—è½¬è¯­éŸ³ï¼ˆåŒæ­¥å°è£…ï¼‰"""
        try:
            return asyncio.run(self.text_to_speech_async(text, voice))
        except Exception as e:
            print(f"Speech synthesis error: {str(e)}")
            return None
    
    def generate_conversation_response(self, user_input, context_docs, mode="conversation", difficulty="intermediate"):
        """ç”Ÿæˆå¯¹è¯å›å¤ - é’ˆå¯¹è‹±è¯­ç»ƒä¹ ä¼˜åŒ–"""
        
        # ç»„åˆçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        context = ""
        if context_docs:
            context = "\n\n".join([f"[Reference {i+1}]\n{doc}" for i, doc in enumerate(context_docs)])
        
        # æ ¹æ®æ¨¡å¼è°ƒæ•´æç¤ºè¯
        system_prompts = {
            "conversation": """You are a friendly English conversation partner helping someone practice English. 
Based on the knowledge base content, engage in natural conversation. 
- Speak naturally and encouragingly
- Use appropriate vocabulary for their level
- Ask follow-up questions to keep conversation flowing
- Correct major errors gently
- Be supportive and patient""",
            
            "roleplay": """You are helping someone practice English through roleplay scenarios.
Based on the knowledge base content (which may describe situations, dialogues, or scenarios):
- Stay in character for the scenario
- Use realistic, situational language
- Provide natural responses as if in a real situation
- Help them practice practical English for real-world use""",
            
            "discussion": """You are an English tutor facilitating topic discussions.
Based on the knowledge base content:
- Discuss the topic in depth
- Ask thought-provoking questions
- Encourage the student to express opinions
- Introduce relevant vocabulary and expressions
- Provide examples and explanations when needed"""
        }
        
        difficulty_notes = {
            "beginner": "Use simple vocabulary and short sentences. Speak slowly and clearly.",
            "intermediate": "Use everyday vocabulary with some advanced words. Speak at normal pace.",
            "advanced": "Use sophisticated vocabulary and complex sentences. Discuss abstract concepts."
        }
        
        # æ„å»ºå¯¹è¯å†å²
        history_text = ""
        if self.conversation_history:
            history_text = "\n\nConversation History:\n"
            for entry in self.conversation_history[-6:]:  # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                history_text += f"Student: {entry['user']}\nTeacher: {entry['assistant']}\n\n"
        
        prompt = f"""{system_prompts.get(mode, system_prompts['conversation'])}

Difficulty Level: {difficulty}
Note: {difficulty_notes.get(difficulty, difficulty_notes['intermediate'])}

Knowledge Base Content:
{context if context else "No specific reference material. Engage in general conversation."}
{history_text}
Student: {user_input}

Teacher (respond in English, naturally and helpfully):"""

        try:
            if LLM_TYPE == "GEMINI":
                response = gemini_model.generate_content(prompt)
                return response.text
            
            elif LLM_TYPE == "OPENAI":
                response = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompts.get(mode, system_prompts['conversation'])},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.8,
                    max_tokens=300
                )
                return response.choices[0].message.content
            
            else:  # OLLAMA
                response = ollama.chat(
                    model='qwen2:7b',
                    messages=[{'role': 'user', 'content': prompt}]
                )
                return response['message']['content']
        
        except Exception as e:
            error_msg = str(e)
            if "api_key" in error_msg.lower() or "API key" in error_msg:
                return f"âŒ API Key not configured.\n\nPlease set: export GOOGLE_API_KEY='your-key'"
            else:
                return f"âŒ LLM Error: {error_msg}"
    
    def extract_text_from_file(self, file_path):
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
        file_ext = Path(file_path).suffix.lower()
        
        try:
            if file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            elif file_ext == '.pdf':
                text = ""
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                return text
            
            elif file_ext in ['.docx', '.doc']:
                doc = docx.Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs])
            
            else:
                return None
        except Exception as e:
            return f"Error: {str(e)}"
    
    def add_document(self, file_path, chunk_size=500):
        """æ·»åŠ å­¦ä¹ ææ–™åˆ°çŸ¥è¯†åº“"""
        if not file_path:
            return "âŒ Please upload a file first"
        
        text = self.extract_text_from_file(file_path)
        if not text:
            return "âŒ Unsupported file format or empty file"
        
        if text.startswith("Error"):
            return text
        
        # åˆ†å—
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunk = text[i:i+chunk_size].strip()
            if chunk:
                chunks.append(chunk)
        
        filename = Path(file_path).name
        current_count = self.collection.count()
        
        ids = [f"{filename}_chunk_{i+current_count}" for i in range(len(chunks))]
        metadatas = [{"source": filename, "chunk_id": i} for i in range(len(chunks))]
        
        self.collection.add(
            documents=chunks,
            ids=ids,
            metadatas=metadatas
        )
        
        return f"âœ… Successfully added {len(chunks)} content blocks\nFile: {filename}"
    
    def practice_conversation(self, user_input, mode, difficulty, voice, n_results=3):
        """è¿›è¡Œå¯¹è¯ç»ƒä¹ """
        if not user_input.strip():
            return "Please say something", None, ""
        
        # æ£€ç´¢ç›¸å…³çŸ¥è¯†åº“å†…å®¹
        context_docs = []
        if self.collection.count() > 0:
            results = self.collection.query(
                query_texts=[user_input],
                n_results=min(n_results, self.collection.count())
            )
            if results['documents'][0]:
                context_docs = results['documents'][0]
        
        # ç”Ÿæˆå›å¤
        response = self.generate_conversation_response(
            user_input, 
            context_docs, 
            mode=mode, 
            difficulty=difficulty
        )
        
        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            "user": user_input,
            "assistant": response
        })
        
        # æ›´æ–°è¯­éŸ³
        self.current_voice = self.tts_voices.get(voice, "en-US-AriaNeural")
        
        # ç”Ÿæˆè¯­éŸ³
        audio_path = self.text_to_speech(response)
        
        # æ„å»ºæ˜¾ç¤ºçš„å¯¹è¯å†å²
        chat_display = ""
        for entry in self.conversation_history[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10è½®
            chat_display += f"**ğŸ§‘ You:** {entry['user']}\n\n"
            chat_display += f"**ğŸ¤– Teacher:** {entry['assistant']}\n\n"
            chat_display += "---\n\n"
        
        return chat_display, audio_path, ""
    
    def practice_with_voice(self, audio, mode, difficulty, voice, n_results=3):
        """è¯­éŸ³å¯¹è¯ç»ƒä¹ """
        if audio is None:
            return "Please record your voice", None, "", ""
        
        # è¯­éŸ³è½¬æ–‡å­—
        user_input = self.transcribe_audio(audio)
        
        if not user_input or user_input.startswith("Error"):
            return user_input, None, "", ""
        
        # è¿›è¡Œå¯¹è¯
        chat_display, audio_path, _ = self.practice_conversation(
            user_input, mode, difficulty, voice, n_results
        )
        
        return chat_display, audio_path, user_input, ""
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯"""
        self.conversation_history = []
        return "âœ… Conversation reset. Ready for a new practice session!"
    
    def get_stats(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡"""
        count = self.collection.count()
        if count == 0:
            return "Knowledge base is empty. Upload some learning materials to get started!"
        
        results = self.collection.get()
        sources = set([m.get('source', 'Unknown') for m in results['metadatas']])
        
        stats = f"ğŸ“Š Knowledge Base Statistics\n\n"
        stats += f"- Total content blocks: {count}\n"
        stats += f"- Source files: {len(sources)}\n"
        stats += f"- Files:\n"
        for source in sources:
            stats += f"  â€¢ {source}\n"
        
        return stats
    
    def clear_database(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            self.client.delete_collection(name="knowledge_base")
            self.collection = self.client.create_collection(
                name="knowledge_base",
                embedding_function=self.embedding_fn
            )
            return "âœ… Knowledge base cleared"
        except Exception as e:
            return f"âŒ Clear failed: {str(e)}"

# åˆå§‹åŒ–AI
ai = EnglishConversationAI()

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="AI è‹±è¯­å£è¯­ç»ƒä¹ åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ™ï¸ AI è‹±è¯­å£è¯­ç»ƒä¹ åŠ©æ‰‹
    
    åŸºäºä½ çš„å­¦ä¹ ææ–™ï¼Œä¸ AI è¿›è¡Œè‹±è¯­å¯¹è¯ç»ƒä¹ ï¼
    
    ğŸ¤– ç”± **Google Gemini 1.5 Flash** é©±åŠ¨
    """)
    
    with gr.Tabs():
        # Tab 1: ä¸Šä¼ å­¦ä¹ ææ–™
        with gr.Tab("ğŸ“¤ ä¸Šä¼ å­¦ä¹ ææ–™"):
            gr.Markdown("""
            ä¸Šä¼ è‹±è¯­å­¦ä¹ ææ–™ï¼Œä¾‹å¦‚ï¼š
            - æ•™æå¯¹è¯
            - è¯é¢˜æ–‡ç« 
            - åœºæ™¯å¯¹è¯
            - è¯æ±‡åˆ—è¡¨
            """)
            
            with gr.Row():
                with gr.Column():
                    file_input = gr.File(
                        label="é€‰æ‹©æ–‡ä»¶",
                        file_types=[".txt", ".pdf", ".docx", ".doc"]
                    )
                    chunk_size = gr.Slider(
                        minimum=200,
                        maximum=1000,
                        value=500,
                        step=50,
                        label="æ–‡æœ¬å—å¤§å°"
                    )
                    upload_btn = gr.Button("ğŸ“ æ·»åŠ åˆ°çŸ¥è¯†åº“", variant="primary")
                
                with gr.Column():
                    upload_output = gr.Textbox(
                        label="ä¸Šä¼ ç»“æœ",
                        lines=5
                    )
            
            upload_btn.click(
                fn=lambda f, c: ai.add_document(f.name if f else None, c),
                inputs=[file_input, chunk_size],
                outputs=upload_output
            )
        
        # Tab 2: è¯­éŸ³å¯¹è¯ç»ƒä¹ 
        with gr.Tab("ğŸ¤ è¯­éŸ³å¯¹è¯"):
            gr.Markdown("### ç‚¹å‡»éº¦å…‹é£æŒ‰é’®ï¼Œç”¨è‹±è¯­è¯´å‡ºä½ æƒ³è¯´çš„è¯")
            
            with gr.Row():
                with gr.Column(scale=1):
                    audio_input = gr.Audio(
                        sources=["microphone"],
                        type="filepath",
                        label="ğŸ¤ å½•åˆ¶è¯­éŸ³"
                    )
                    
                    mode_voice = gr.Radio(
                        choices=["conversation", "roleplay", "discussion"],
                        value="conversation",
                        label="ç»ƒä¹ æ¨¡å¼",
                        info="å¯¹è¯ï¼šè‡ªç”±èŠå¤© | è§’è‰²æ‰®æ¼”ï¼šåœºæ™¯ç»ƒä¹  | è®¨è®ºï¼šè¯é¢˜è®¨è®º"
                    )
                    
                    difficulty_voice = gr.Radio(
                        choices=["beginner", "intermediate", "advanced"],
                        value="intermediate",
                        label="éš¾åº¦çº§åˆ«"
                    )
                    
                    voice_select_voice = gr.Radio(
                        choices=list(ai.tts_voices.keys()),
                        value="ç¾å¼å¥³å£°",
                        label="è€å¸ˆè¯­éŸ³"
                    )
                    
                    voice_btn = gr.Button("ğŸ—£ï¸ å¼€å§‹ç»ƒä¹ ", variant="primary", size="lg")
                    reset_btn_voice = gr.Button("ğŸ”„ é‡ç½®å¯¹è¯", variant="secondary")
                
                with gr.Column(scale=2):
                    recognized_voice = gr.Textbox(
                        label="ğŸ“ ä½ è¯´çš„å†…å®¹",
                        lines=2
                    )
                    conversation_display_voice = gr.Textbox(
                        label="ğŸ’¬ å¯¹è¯å†å²",
                        lines=12
                    )
                    audio_output_voice = gr.Audio(
                        label="ğŸ”Š è€å¸ˆçš„å›å¤",
                        autoplay=True
                    )
            
            voice_btn.click(
                fn=ai.practice_with_voice,
                inputs=[audio_input, mode_voice, difficulty_voice, voice_select_voice],
                outputs=[conversation_display_voice, audio_output_voice, recognized_voice, audio_input]
            )
            
            reset_btn_voice.click(
                fn=ai.reset_conversation,
                outputs=conversation_display_voice
            )
        
        # Tab 3: æ–‡å­—å¯¹è¯ç»ƒä¹ 
        with gr.Tab("ğŸ’¬ æ–‡å­—å¯¹è¯"):
            with gr.Row():
                with gr.Column(scale=1):
                    text_input = gr.Textbox(
                        label="ç”¨è‹±è¯­è¾“å…¥ä½ çš„æ¶ˆæ¯",
                        placeholder="Hello! I'd like to practice English conversation...",
                        lines=3
                    )
                    
                    mode_text = gr.Radio(
                        choices=["conversation", "roleplay", "discussion"],
                        value="conversation",
                        label="ç»ƒä¹ æ¨¡å¼"
                    )
                    
                    difficulty_text = gr.Radio(
                        choices=["beginner", "intermediate", "advanced"],
                        value="intermediate",
                        label="éš¾åº¦çº§åˆ«"
                    )
                    
                    voice_select_text = gr.Radio(
                        choices=list(ai.tts_voices.keys()),
                        value="ç¾å¼å¥³å£°",
                        label="è€å¸ˆè¯­éŸ³"
                    )
                    
                    text_btn = gr.Button("ğŸ’¬ å‘é€", variant="primary", size="lg")
                    reset_btn_text = gr.Button("ğŸ”„ é‡ç½®å¯¹è¯", variant="secondary")
                
                with gr.Column(scale=2):
                    conversation_display_text = gr.Textbox(
                        label="ğŸ’¬ å¯¹è¯å†å²",
                        lines=15
                    )
                    audio_output_text = gr.Audio(
                        label="ğŸ”Š è€å¸ˆçš„å›å¤",
                        autoplay=True
                    )
            
            gr.Examples(
                examples=[
                    ["Hello! How are you today?"],
                    ["Can you help me practice ordering food at a restaurant?"],
                    ["What do you think about artificial intelligence?"],
                    ["I'd like to discuss environmental issues."],
                ],
                inputs=text_input
            )
            
            text_btn.click(
                fn=ai.practice_conversation,
                inputs=[text_input, mode_text, difficulty_text, voice_select_text],
                outputs=[conversation_display_text, audio_output_text, text_input]
            )
            
            reset_btn_text.click(
                fn=ai.reset_conversation,
                outputs=conversation_display_text
            )
        
        # Tab 4: çŸ¥è¯†åº“ç®¡ç†
        with gr.Tab("âš™ï¸ çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                stats_btn = gr.Button("ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", variant="stop")
            
            stats_output = gr.Textbox(
                label="ç»Ÿè®¡ä¿¡æ¯",
                lines=10
            )
            
            stats_btn.click(
                fn=ai.get_stats,
                outputs=stats_output
            )
            
            clear_btn.click(
                fn=ai.clear_database,
                outputs=stats_output
            )
    
    gr.Markdown("""
    ---
    ### ğŸ’¡ ä½¿ç”¨æ–¹æ³•
    
    1. **ä¸Šä¼ å­¦ä¹ ææ–™**ï¼šæ·»åŠ è‹±è¯­å­¦ä¹ èµ„æ–™ï¼ˆå¯¹è¯ã€æ–‡ç« ã€åœºæ™¯ç­‰ï¼‰
    2. **é€‰æ‹©æ¨¡å¼**ï¼š
       - ğŸ—£ï¸ **å¯¹è¯ (Conversation)**ï¼šè‡ªç„¶çš„è‡ªç”±å¯¹è¯
       - ğŸ­ **è§’è‰²æ‰®æ¼” (Roleplay)**ï¼šç»ƒä¹ çœŸå®åœºæ™¯ï¼ˆé¤å…ã€è´­ç‰©ç­‰ï¼‰
       - ğŸ’­ **è®¨è®º (Discussion)**ï¼šæ·±å…¥è®¨è®ºè¯é¢˜
    3. **é€‰æ‹©éš¾åº¦**ï¼šåˆçº§ (Beginner)ã€ä¸­çº§ (Intermediate)ã€é«˜çº§ (Advanced)
    4. **å¼€å§‹ç»ƒä¹ **ï¼šç”¨è‹±è¯­è¯´è¯æˆ–æ‰“å­—ï¼ŒAI ä¼šè‡ªç„¶å›å¤
    
    ### ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹
    
    - âœ… åŸºäºä½ çš„å­¦ä¹ ææ–™è¿›è¡Œè‡ªç„¶è‹±è¯­å¯¹è¯
    - âœ… å¤šä¸ªéš¾åº¦çº§åˆ«å¯é€‰
    - âœ… ä¸åŒçš„ç»ƒä¹ æ¨¡å¼
    - âœ… è¯­éŸ³è¯†åˆ«å’Œåˆæˆ
    - âœ… å¯¹è¯å†å²è®°å½•
    - âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›å¤
    
    ### ğŸ”§ é…ç½®è¯´æ˜
    
    ```bash
    export GOOGLE_API_KEY="ä½ çš„key"
    ```
    åœ¨è¿™é‡Œå…è´¹è·å–ï¼šhttps://makersuite.google.com/app/apikey
    """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )